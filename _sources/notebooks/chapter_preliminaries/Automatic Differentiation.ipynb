{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e56df966-3e26-4ef0-88ec-abb4f80050f6",
   "metadata": {},
   "source": [
    "# Automatic Differentiation\n",
    "\n",
    "Recall from Section 2.4 that calculating derivatives is the crucial step in all of the optimization algorithms that we will use to train deep networks. While the calculations are straightforward, working them out by hand can be tedious and error-prone, and this problem only grows as our models become more complex.\n",
    "\n",
    "Fortunately all modern deep learning frameworks take this work off of our plates by offering automatic differentiation (often shortened to autograd). As we pass data through each successive function, the framework builds a computational graph that tracks how each value depends on others. To calculate derivatives, automatic differentiation works backwards through this graph applying the chain rule. The computational algorithm for applying the chain rule in this fashion is called backpropagation.\n",
    "\n",
    "While autograd libraries have become a hot concern over the past decade, they have a long history. In fact the earliest references to autograd date back over half of a century (Wengert, 1964). The core ideas behind modern backpropagation date to a PhD thesis from 1980 (Speelpenning, 1980) and were further developed in the late 1980s (Griewank, 1989). While backpropagation has become the default method for computing gradients, it is not the only option. For instance, the Julia programming language employs forward propagation (Revels et al., 2016). Before exploring methods, let’s first master the [Zygote](https://fluxml.ai/Zygote.jl/) package.\n",
    "\n",
    "## A Simple Function\n",
    "\n",
    "To start, we assign x an initial value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92874c27-c7a4-4692-b7f7-876f6200ea1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " 1.0\n",
       " 2.0\n",
       " 3.0\n",
       " 4.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = collect(1.0:4.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a564a2b-0d21-4e78-9a59-91662651e494",
   "metadata": {},
   "source": [
    "We now define a function of x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f9424c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "y(x) = 2x⋅x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4a6eed6",
   "metadata": {},
   "source": [
    "We can now take the gradient of y with respect to x by calling `gradient` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f5bc8a5-2bf7-469d-b2e2-24899c79023f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([4.0, 8.0, 12.0, 16.0],)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Zygote\n",
    "\n",
    "g = gradient(y,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b52f89-9b82-4bc9-8460-5487224bce74",
   "metadata": {},
   "source": [
    "We can now verify that the automatic gradient computation and the expected result are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8aa8381-bca9-4980-8a89-8358741bf1be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first(g) == x.*4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3052c21-76df-49b3-a588-6ed05759dbb2",
   "metadata": {},
   "source": [
    "Now let’s calculate another function of x and take its gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d86607be-ca25-4d9f-bfa6-601f3850c67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Fill(1.0, 4),)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y(x) = sum(x)\n",
    "gradient(y,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43116c42-94bb-4136-bd3a-acc00096a28c",
   "metadata": {},
   "source": [
    "## Backward for Non-Scalar Variables\n",
    "\n",
    "When y is a vector, the most natural interpretation of the derivative of y with respect to a vector x is a matrix called the Jacobian that contains the partial derivatives of each component of y with respect to each component of x. Likewise, for higher-order y and x, the differentiation result could be an even higher-order tensor.\n",
    "\n",
    "While Jacobians do show up in some advanced machine learning techniques, more commonly we want to sum up the gradients of each component of y with respect to the full vector x, yielding a vector of the same shape as x. For example, we often have a vector representing the value of our loss function calculated separately for each example among a batch of training examples. Here, we just want to sum up the gradients computed individually for each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5e2ab39-e9b0-4250-8863-2c28e242979f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2.0, 4.0, 6.0, 8.0],)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y(x) = x.*x\n",
    "gradient(x->sum(y(x)),x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.1",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
