{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a47f978-a446-42f7-9b72-585d45a4d0c8",
   "metadata": {},
   "source": [
    "# Concise Implementation of Softmax Regression\n",
    "Just as high-level deep learning frameworks made it easier to implement linear regression (see Section 3.5), they are similarly convenient here.\n",
    "\n",
    "## Defining the Model\n",
    "\n",
    "As in Section 3.5, we construct our fully connected layer using the built-in layer. We use a `flatten` layer to reshape arbitrarly-shaped input into a matrix-shaped output, preserving the size of the last dimension.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "216f6688-aebe-461d-ba07-b8617cc25b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Flux.flatten,\n",
       "  Dense(784 => 10),                     \u001b[90m# 7_850 parameters\u001b[39m\n",
       ") "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux\n",
    "model = Chain(Flux.flatten,Dense(28*28=>10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75088c25-3917-425b-bb3a-78d647c98eaa",
   "metadata": {},
   "source": [
    "\n",
    "This avoids both overflow and underflow. We will want to keep the conventional softmax function handy in case we ever want to evaluate the output probabilities by our model. But instead of passing softmax probabilities into our new loss function, we just pass the logits and compute the softmax and its log all at once inside the cross-entropy loss function, which does smart things like the “LogSumExp trick”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d0e910e-d62f-4af7-b67e-d5c50ba47e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(model,x,y) = Flux.logitcrossentropy(model(x),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ca74dd-eec9-4cbf-8330-0ef00b326535",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Next we train our model. We use Fashion-MNIST images, flattened to 784-dimensional feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f7bf6ad-394e-41c5-a9c8-91c16730d00b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Float32}:\n",
       " 0.6156508\n",
       " 0.54177076\n",
       " 0.50940657\n",
       " 0.48994175\n",
       " 0.47646806\n",
       " 0.46637794\n",
       " 0.45843133\n",
       " 0.4519492\n",
       " 0.44652238\n",
       " 0.44188702"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MLUtils\n",
    "using MLDatasets\n",
    "\n",
    "ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = true\n",
    "mnist_train,mnist_test = FashionMNIST(:train),FashionMNIST(:test)\n",
    "features = mnist_train.features\n",
    "labels = Flux.onehotbatch(mnist_train.targets,0:9)\n",
    "\n",
    "train_loader = DataLoader((features,labels),batchsize=256)\n",
    "num_epochs = 10\n",
    "loss_volume = map(1:num_epochs) do i\n",
    "    for data in train_loader\n",
    "        Flux.train!(loss,model,[data],Descent())\n",
    "    end\n",
    "    loss(model,features,labels)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af25f1-c360-47f1-b9a1-7a6799cf9f38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.0",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
