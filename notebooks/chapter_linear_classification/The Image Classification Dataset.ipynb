{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85921704-8b3b-4d0a-83eb-d37b63268e83",
   "metadata": {},
   "source": [
    "# The Image Classification Dataset\n",
    "\n",
    "One of the widely used dataset for image classification is the MNIST dataset (LeCun et al., 1998) of handwritten digits. At the time of its release in the 1990s it posed a formidable challenge to most machine learning algorithms, consisting of 60,000 images of 28x28 pixels resolution (plus a test dataset of 10,000 images). To put things into perspective, at the time, a Sun SPARCStation 5 with a whopping 64MB of RAM and a blistering 5 MFLOPs was considered state of the art equipment for machine learning at AT&T Bell Laboratories in 1995. Achieving high accuracy on digit recognition was a key component in automating letter sorting for the USPS in the 1990s. Deep networks such as LeNet-5 (LeCun et al., 1995), support vector machines with invariances (Schölkopf et al., 1996), and tangent distance classifiers (Simard et al., 1998) all allowed to reach error rates below 1%.\n",
    "\n",
    "For over a decade, MNIST served as the point of reference for comparing machine learning algorithms. While it had a good run as a benchmark dataset, even simple models by today’s standards achieve classification accuracy over 95%, making it unsuitable for distinguishing between stronger models and weaker ones. Even more so, the dataset allows for very high levels of accuracy, not typically seen in many classification problems. This skewed algorithmic development towards specific families of algorithms that can take advantage of clean datasets, such as active set methods and boundary-seeking active set algorithms. Today, MNIST serves as more of sanity checks than as a benchmark. ImageNet (Deng et al., 2009) poses a much more relevant challenge. Unfortunately, ImageNet is too large for many of the examples and illustrations in this book, as it would take too long to train to make the examples interactive. As a substitute we will focus our discussion in the coming sections on the qualitatively similar, but much smaller Fashion-MNIST dataset (Xiao et al., 2017), which was released in 2017. It contains images of 10 categories of clothing at 28x28 pixels resolution.\n",
    "\n",
    "## Loading the Dataset\n",
    "\n",
    "Since it is such a frequently used dataset, all major frameworks provide preprocessed versions of it. We can download and read the Fashion-MNIST dataset into memory using `MLDatasets.jl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5921ff07-110c-41c1-ba20-c74ce6417424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset FashionMNIST:\n",
       "  metadata  =>    Dict{String, Any} with 4 entries\n",
       "  split     =>    :train\n",
       "  features  =>    28×28×60000 Array{Float32, 3}\n",
       "  targets   =>    60000-element Vector{Int64}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using MLDatasets\n",
    "\n",
    "# ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = true\n",
    "# FashionMNIST(; Tx=Float32, split=:train)\n",
    "# length(train_x)#,length(text_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931d8f93-ecac-4d5d-931a-619a2fc75f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.0",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
