{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85921704-8b3b-4d0a-83eb-d37b63268e83",
   "metadata": {},
   "source": [
    "# The Image Classification Dataset\n",
    "\n",
    "One of the widely used dataset for image classification is the MNIST dataset (LeCun et al., 1998) of handwritten digits. At the time of its release in the 1990s it posed a formidable challenge to most machine learning algorithms, consisting of 60,000 images of 28x28 pixels resolution (plus a test dataset of 10,000 images). To put things into perspective, at the time, a Sun SPARCStation 5 with a whopping 64MB of RAM and a blistering 5 MFLOPs was considered state of the art equipment for machine learning at AT&T Bell Laboratories in 1995. Achieving high accuracy on digit recognition was a key component in automating letter sorting for the USPS in the 1990s. Deep networks such as LeNet-5 (LeCun et al., 1995), support vector machines with invariances (Schölkopf et al., 1996), and tangent distance classifiers (Simard et al., 1998) all allowed to reach error rates below 1%.\n",
    "\n",
    "For over a decade, MNIST served as the point of reference for comparing machine learning algorithms. While it had a good run as a benchmark dataset, even simple models by today’s standards achieve classification accuracy over 95%, making it unsuitable for distinguishing between stronger models and weaker ones. Even more so, the dataset allows for very high levels of accuracy, not typically seen in many classification problems. This skewed algorithmic development towards specific families of algorithms that can take advantage of clean datasets, such as active set methods and boundary-seeking active set algorithms. Today, MNIST serves as more of sanity checks than as a benchmark. ImageNet (Deng et al., 2009) poses a much more relevant challenge. Unfortunately, ImageNet is too large for many of the examples and illustrations in this book, as it would take too long to train to make the examples interactive. As a substitute we will focus our discussion in the coming sections on the qualitatively similar, but much smaller Fashion-MNIST dataset (Xiao et al., 2017), which was released in 2017. It contains images of 10 categories of clothing at 28x28 pixels resolution.\n",
    "\n",
    "## Loading the Dataset\n",
    "\n",
    "Since it is such a frequently used dataset, all major frameworks provide preprocessed versions of it. We can download and read the Fashion-MNIST dataset into memory using `MLDatasets.jl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5921ff07-110c-41c1-ba20-c74ce6417424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset FashionMNIST:\n",
       "  metadata  =>    Dict{String, Any} with 4 entries\n",
       "  split     =>    :test\n",
       "  features  =>    28×28×10000 Array{Float32, 3}\n",
       "  targets   =>    10000-element Vector{Int64}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MLDatasets\n",
    "\n",
    "ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = true\n",
    "mnist_train = FashionMNIST(:train)\n",
    "mnist_test = FashionMNIST(:test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4b1c7c-2dd2-4035-babb-4ef3585744f1",
   "metadata": {},
   "source": [
    "Fashion-MNIST consists of images from 10 categories, each represented by 6,000 images in the training dataset and by 1,000 in the test dataset. A test dataset is used for evaluating model performance (it must not be used for training). Consequently the training set and the test set contain 60,000 and 10,000 images, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a1530bb-79a1-4af9-9837-24b9a4ad0c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length(mnist_train), length(mnist_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67be4a90-4857-45b2-a53b-98a3d9810f78",
   "metadata": {},
   "source": [
    "This is similar to the original MNIST dataset which consisted of (binary) black and white images. Note, though, that most modern image data which has 3 channels (red, green, blue) and hyperspectral images which can have in excess of 100 channels (the HyMap sensor has 126 channels). By convention we store image as a `h x w` array, `h` is the height and `w` is the width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bbf8310-2420-4c90-8ab0-6bcea7691008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(mnist_train.features[:,:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438df08f-2d21-4fcb-9a49-2208048a7cb7",
   "metadata": {},
   "source": [
    "The categories of Fashion-MNIST have human-understandable names. The following code converts between numeric labels and their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef02b6a9-e556-4546-86cb-4d98fc5942ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_fashion_mnist_labels (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_name =  [\"T-Shirt\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "get_fashion_mnist_labels(label::Int) = class_name[label+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeb04bc-0415-4b6a-bc9c-1dab9da2e27c",
   "metadata": {},
   "source": [
    "## Reading a Minibatch\n",
    "\n",
    "To see how this works, let’s load a minibatch of images by invoking the `DataLoader` method. It contains 18 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f594b75b-c66f-49f9-b3e8-544debe244d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3334-element DataLoader(::Tuple{Array{Float32, 3}, Vector{Int64}}, shuffle=true, batchsize=18)\n",
       "  with first element:\n",
       "  (28×28×18 Array{Float32, 3}, 18-element Vector{Int64},)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MLUtils\n",
    "train_loader = DataLoader((mnist_train.features,mnist_train.targets),batchsize=18,shuffle=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ea1fe6-1eb7-430f-acb0-4ee4b7884f6f",
   "metadata": {},
   "source": [
    "Let’s look at the time it takes to read the images. Even though it is a built-in loader, it is not blazingly fast. Nonetheless, this is sufficient since processing images with a deep network takes quite a bit longer. Hence it is good enough that training a network will not be IO constrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54b66da2-9eec-488d-ac0a-f626862eaa06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.089176 seconds (51.81 k allocations: 183.673 MiB, 16.84% gc time, 15.37% compilation time)\n"
     ]
    }
   ],
   "source": [
    "@time for (X,y) in train_loader\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02aa739-4b75-4be3-9b2f-0d99a3d5f289",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "In general, it is a good idea to visualize and inspect data that you’re training on. Humans are very good at spotting unusual aspects and as such, visualization serves as an additional safeguard against mistakes and errors in the design of experiments. Here are the images and their corresponding labels (in text) for the first few examples in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faa01ffa-4db0-4428-be2f-3708ebdb84f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using CairoMakie\n",
    "# using ImageShow\n",
    "\n",
    "# function show_images(features::Array{<:Real,3},targets::Vector{Int64})\n",
    "#     column_num = 9\n",
    "#     batch_num = size(features)[3]\n",
    "#     row_num = ceil(batch_num / column_num)\n",
    "#     # fig = Figure()(resolution = (28*1.5*column_num,28*1.5*row_num))\n",
    "#     fig = Figure()\n",
    "#     image(fig[1, 1], rotr90(convert2image(FashionMNIST,features[:,:,1])), axis = (title = \"$(get_fashion_mnist_labels(targets[1]))\",))\n",
    "#     fig\n",
    "# end\n",
    "# minist_batch = mnist_train[1:18]\n",
    "\n",
    "# show_images(minist_batch.features,minist_batch.targets)\n",
    "# convert2image(mnist_train,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.0",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
